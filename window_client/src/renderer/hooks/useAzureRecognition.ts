import { useRef, useCallback } from 'react';
import * as sdk from 'microsoft-cognitiveservices-speech-sdk';
import {
  getAzureSpeechConfig,
  createPushAudioInputStream,
} from '../lib/azureSpeech';

export interface RecognitionTextMessage {
  role: 'interviewee' | 'interviewer';
  text: string;
  type: 'recognizing' | 'recognized';
  status: 'pending' | 'received';
  timestamp: number;
}

export interface UseAzureRecognitionResult {
  startRecognition: (onTextReceived: (message: RecognitionTextMessage) => void) => void;
  stopRecognition: () => void;
  sendAudioData: (role: 'interviewee' | 'interviewer', audioData: Int16Array) => void;
}

/**
 * Azure åŒæµè¯­éŸ³è¯†åˆ« Hookï¼ˆElectron ç«¯ï¼‰
 */
export function useAzureRecognition(): UseAzureRecognitionResult {
  const intervieweeRecognizerRef = useRef<sdk.SpeechRecognizer | null>(null);
  const interviewerRecognizerRef = useRef<sdk.SpeechRecognizer | null>(null);

  const intervieweePushStreamRef = useRef<sdk.PushAudioInputStream | null>(null);
  const interviewerPushStreamRef = useRef<sdk.PushAudioInputStream | null>(null);

  const textCallbackRef = useRef<((message: RecognitionTextMessage) => void) | null>(null);

  /**
   * åˆ›å»ºè¯†åˆ«å™¨
   */
  const createRecognizer = (
    role: 'interviewee' | 'interviewer'
  ): sdk.SpeechRecognizer => {
    console.log(`[Azure Recognition] åˆ›å»º ${role === 'interviewee' ? 'é¢è¯•è€…' : 'é¢è¯•å®˜'} è¯†åˆ«å™¨...`);

    const speechConfig = getAzureSpeechConfig();
    const pushStream = createPushAudioInputStream();
    const audioConfig = sdk.AudioConfig.fromStreamInput(pushStream);
    const recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);

    // ä¿å­˜ PushStream å¼•ç”¨
    if (role === 'interviewee') {
      intervieweePushStreamRef.current = pushStream;
    } else {
      interviewerPushStreamRef.current = pushStream;
    }

    // è¯†åˆ«ä¸­äº‹ä»¶
    recognizer.recognizing = (_s, e) => {
      if (e.result.reason === sdk.ResultReason.RecognizingSpeech && e.result.text && textCallbackRef.current) {
        console.log(`[Azure Recognition] ${role === 'interviewee' ? 'ğŸ¤' : 'ğŸ‘”'} [recognizing] [pending]:`, e.result.text);
        
        textCallbackRef.current({
          role,
          text: e.result.text,
          type: 'recognizing',
          status: 'pending',
          timestamp: Date.now()
        });
      }
    };

    // è¯†åˆ«å®Œæˆäº‹ä»¶
    recognizer.recognized = (_s, e) => {
      if (e.result.reason === sdk.ResultReason.RecognizedSpeech && e.result.text && textCallbackRef.current) {
        console.log(`[Azure Recognition] ${role === 'interviewee' ? 'ğŸ¤' : 'ğŸ‘”'} [recognized] [received]:`, e.result.text);
        
        textCallbackRef.current({
          role,
          text: e.result.text,
          type: 'recognized',
          status: 'received',
          timestamp: Date.now()
        });
      }
    };

    // å–æ¶ˆäº‹ä»¶
    recognizer.canceled = (_s, e) => {
      console.error(`[Azure Recognition] ${role} è¯†åˆ«å–æ¶ˆ:`, sdk.CancellationReason[e.reason]);
      if (e.errorDetails) {
        console.error('[Azure Recognition] é”™è¯¯è¯¦æƒ…:', e.errorDetails);
      }
    };

    return recognizer;
  };

  /**
   * å¯åŠ¨åŒæµè¯†åˆ«
   */
  const startRecognition = useCallback((onTextReceived: (message: RecognitionTextMessage) => void) => {
    console.log('â•'.repeat(50));
    console.log('[Azure Recognition] ğŸ™ï¸ å¯åŠ¨åŒæµè¯†åˆ«æ¨¡å¼ï¼ˆElectron ç«¯ï¼‰');

    textCallbackRef.current = onTextReceived;

    // åˆ›å»ºé¢è¯•è€…è¯†åˆ«å™¨
    const intervieweeRecognizer = createRecognizer('interviewee');
    intervieweeRecognizerRef.current = intervieweeRecognizer;
    intervieweeRecognizer.startContinuousRecognitionAsync(
      () => {
        console.log('[Azure Recognition] ğŸ¤ é¢è¯•è€…è¯†åˆ«å™¨å·²å¯åŠ¨');
      },
      (error) => {
        console.error('[Azure Recognition] é¢è¯•è€…è¯†åˆ«å™¨å¯åŠ¨å¤±è´¥:', error);
      }
    );

    // åˆ›å»ºé¢è¯•å®˜è¯†åˆ«å™¨
    const interviewerRecognizer = createRecognizer('interviewer');
    interviewerRecognizerRef.current = interviewerRecognizer;
    interviewerRecognizer.startContinuousRecognitionAsync(
      () => {
        console.log('[Azure Recognition] ğŸ‘” é¢è¯•å®˜è¯†åˆ«å™¨å·²å¯åŠ¨');
        console.log('â•'.repeat(50));
        console.log('[Azure Recognition] âœ… åŒæµè¯†åˆ«å·²å…¨éƒ¨å¯åŠ¨');
        console.log('â•'.repeat(50));
      },
      (error) => {
        console.error('[Azure Recognition] é¢è¯•å®˜è¯†åˆ«å™¨å¯åŠ¨å¤±è´¥:', error);
      }
    );
  }, []);

  /**
   * åœæ­¢è¯†åˆ«
   */
  const stopRecognition = useCallback(() => {
    console.log('[Azure Recognition] åœæ­¢è¯†åˆ«');

    // åœæ­¢é¢è¯•è€…è¯†åˆ«å™¨
    if (intervieweeRecognizerRef.current) {
      intervieweeRecognizerRef.current.stopContinuousRecognitionAsync(
        () => {
          if (intervieweeRecognizerRef.current) {
            intervieweeRecognizerRef.current.close();
            intervieweeRecognizerRef.current = null;
          }
          if (intervieweePushStreamRef.current) {
            intervieweePushStreamRef.current.close();
            intervieweePushStreamRef.current = null;
          }
        },
        (error) => {
          console.error('[Azure Recognition] é¢è¯•è€…è¯†åˆ«å™¨åœæ­¢å¤±è´¥:', error);
        }
      );
    }

    // åœæ­¢é¢è¯•å®˜è¯†åˆ«å™¨
    if (interviewerRecognizerRef.current) {
      interviewerRecognizerRef.current.stopContinuousRecognitionAsync(
        () => {
          if (interviewerRecognizerRef.current) {
            interviewerRecognizerRef.current.close();
            interviewerRecognizerRef.current = null;
          }
          if (interviewerPushStreamRef.current) {
            interviewerPushStreamRef.current.close();
            interviewerPushStreamRef.current = null;
          }
        },
        (error) => {
          console.error('[Azure Recognition] é¢è¯•å®˜è¯†åˆ«å™¨åœæ­¢å¤±è´¥:', error);
        }
      );
    }

    textCallbackRef.current = null;
  }, []);

  /**
   * å‘é€éŸ³é¢‘æ•°æ®
   */
  const sendAudioData = useCallback((role: 'interviewee' | 'interviewer', audioData: Int16Array) => {
    const pushStream = role === 'interviewee' 
      ? intervieweePushStreamRef.current 
      : interviewerPushStreamRef.current;

    if (pushStream) {
      // å°† Int16Array è½¬ä¸º ArrayBuffer å¹¶æ¨é€
      const buffer = Buffer.from(audioData.buffer);
      pushStream.write(buffer.buffer as ArrayBuffer);
    }
  }, []);

  return {
    startRecognition,
    stopRecognition,
    sendAudioData
  };
}

